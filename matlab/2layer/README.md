In this repository there are some scripts to dfine, train and evaluate a 2-layer ANN. The architecture is fixed by using the ReLU as activation for the hidden layer, whereas the output is linear. The number of hidden units is modifiable. All the elements to computing the model are saved in an global variable `struct`.

# nnInit(hiddenLayerSize,inputSize,outputSize)
