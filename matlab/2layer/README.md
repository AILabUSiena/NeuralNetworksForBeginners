In this repository there are some scripts to dfine, train and evaluate a 2-layer ANN. The architecture is fixed by using the ReLU as activation for the hidden layer, whereas the output is linear. The number of hidden units is modifiable. All the elements to computing the model are saved in an global variable `struct` generated by the function [nnInit.m](https://github.com/alered87/First-Day-at-AI-lab/blob/master/matlab/2layer/nnInit.m). For example, if we wnat to process a 2-Dimensional input, producing 1-Dimensional output using 10 hidden units we can declare a structure `nn` by

```matlab
 nn nnInit(hiddenLayerSize,inputSize,outputSize)
 ```
